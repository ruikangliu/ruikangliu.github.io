<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Decoupling Representation and Classifier for Long-Tailed Recognition</title>
    <link href="/2021/11/02/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/"/>
    <url>/2021/11/02/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/</url>
    
    <content type="html"><![CDATA[<ul><li>图片特征的分布和类别标注的分布本质上是不耦合的，任何不均衡分类数据集的再平衡本质上都应该只是对分类器的再均衡，而不应该用类别的分布改变特征学习时图片特征的分布。<span id="more"></span>也就是说，模型完全可以从类别不均衡的数据集中学得样本特征，类别的不均衡只会对分类器产生影响，re-sampling、re-weighting 等方法本质上都应该去平衡分类器的决策边界，而不应该去影响样本特征的学习</li></ul><h1>Introduction</h1><ul><li><p>One typical classifier includes a feature extractor (i.e. backbone) $f(x; θ) = z$ and a classifier $g(z) = W^Tz + b$, which can be formulated as $\tilde y = \arg \max g(z)$. Most of the existing solutions for long-tailed recognition adhere to the scheme of jointly learning representations ($\theta$) and classifiers ($W,b$). However, this paper devised a scheme that <strong>decouple the learning procedure into representation learning and classification</strong>, which set new <strong>SOTA</strong> on common long-tailed benchmarks like ImageNet-LT, Places-LT and iNaturalist. In the experiment, the authors also found surprising results that <strong>instance-balanced sampling learns the best and most generalizable representations</strong>, which challenges common beliefs.</p></li><li><p>What is more, this paper also proposed several <strong>learning methods for classification</strong> used in the decoupled learning scheme, including  1) re-training the parametric linear classifier in a class-balancing manner (i.e., <strong>re-sampling</strong>); 2) <strong>non-parametric nearest class mean classifier</strong>, which classifies the data based on their closest class-specific mean representations from the training set; and 3) <strong>normalizing the classifier weights</strong>, which adjusts the weight magnitude directly to be more balanced, adding a temperature to modulate the normalization procedure.</p></li></ul><h1>Learning Representations for long-tailed recognition</h1><ul><li><p><strong>Sampling strategies</strong>: Instance-balanced sampling, Class-balanced sampling, Square-root sampling, Progressively-balanced sampling</p></li><li><p><strong>Loss re-weighting strategies</strong></p></li></ul><h1>Classification for long-tailed recognition</h1><ul><li><p><strong>Classifier Re-training (cRT)</strong>: Re-train the classifier with class-balanced sampling. That is, keeping the representations fixed, we randomly re-initialize and optimize the classifier weights $W$ and $b$ for a small number of epochs using class-balanced sampling.</p></li><li><p><strong>Nearest Class Mean classifier (NCM)</strong>: First compute the mean feature representation for each class on the training set and then perform nearest neighbor search either using cosine similarity or the Euclidean distance computed on $L_2$ normalized mean features. Despite its simplicity, this is a strong baseline; the cosine similarity alleviates the weight imbalance problem via its inherent normalization.</p></li><li><p>$τ$-normalized classifier (<strong>$τ$-normalized</strong>). We investigate an efficient approach to re-balance the decision boundaries of classifiers, inspired by an empirical observation: After joint training with instance-balanced sampling, the norms of the weights $||w_j||$ are correlated with the cardinality of the classes $n_j$, while, after fine-tuning the classifiers using class-balanced sampling, the norms of the classifier weights tend to be more similar. <strong>Inspired by the above observations</strong>, we scale the weights of $W$ to get $\tilde W = {\tilde w_j }$ by:<br>$$<br>\tilde w_i=f_i*w_i,\text{where }f_i=\frac{1}{||w_i||^\tau}<br>$$<br>where $τ$ is a hyper-parameter controlling the “temperature” of the normalization.  We empirically choose $τ ∈ (0, 1)$ such that the weights can be rectified smoothly. After $τ$-normalization, the classification logits are given by $\hat y = \tilde W^Tf(x; θ)$. Note that we discard the bias term $b$ here due to its negligible effect on the logits and final predictions.</p></li><li><p><strong>Learnable weight scaling (LWS)</strong>: Although for $τ$-normalized in general $τ$ is chosen through cross-validation, we further investigate <strong>learning $f_i$ on the training set, using class-balanced sampling</strong>. In this case, we keep both the representations and classifier weights fixed and only learn the scaling factors $f_i$.</p></li></ul><blockquote><p><strong>Performance drops when a deeper classifier is used.</strong> (1 layer is the best) The author explains that it means the backbone network is enough to learn discriminative representation.</p></blockquote><h1>Experiments</h1><h2 id="Experiment">Experiment</h2><ul><li><strong>Datasets</strong>: Places-LT, ImageNet-LT, iNaturalist 2018.</li><li><strong>Evaluation Protocol</strong>: (1) $All$ (top-1 accuracy over all classes). To better examine performance variations across classes with different number of examples, we further report accuracy on three splits of the set of classes: (2) $Many$-$shot$ (more than 100 images), (3) $Medium$-$shot$ (20∼100 images) and (4) $Few$-$shot$ (less than 20 images).</li><li><strong>Implementation</strong>:<ul><li><strong>backbone</strong>:<ul><li>Places-LT: ResNet-152 (pretrained on ImageNet-2012)</li><li>ImageNet-LT: ResNet-{10,50,101,152} and ResNeXt-{50,101,152}(32x4d)</li><li>iNaturalist 2018: ResNet-{50,101,152}</li></ul></li><li><strong>others</strong>: SGD <strong>optimizer</strong> with momentum 0.9, batch size 512, cosine learning rate schedule (<a href="https://arxiv.org/abs/1608.03983v3">Loshchilov &amp; Hutter, 2016</a>) gradually decaying from 0.2 to 0 and <strong>image resolution</strong> 224 × 224. In the first representation learning stage, the backbone network is usually trained for 90 epochs. In the second stage, i.e., for retraining a classifier (cRT), we restart the learning rate and train it for 10 epochs while keeping the backbone network fixed.</li></ul></li></ul><h2 id="Sampling-strategies-and-decoupled-learning">Sampling strategies and decoupled learning</h2><ul><li>We first train models to learn representations with different sampling strategies. Next, we study three different basic approaches to obtain a classifier with balanced decision boundaries, on top of the learned representations.</li></ul><hr><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-180820.png" alt=""></p><p><strong>Key observations</strong>:</p><ul><li><strong>Sampling matters when training jointly</strong>. From the Joint results in Figure 1 across sampling methods and splits, we see consistent gains in performance when using better sampling strategies. The trends are consistent for the overall performance as well as the medium- and few-shot classes, with <strong>progressively-balanced sampling giving the best results</strong>.</li><li><strong>Decoupling representation and classifier is desirable for long-tailed recognition</strong>. For most cases presented in Figure 1, performance using decoupled methods is significantly better in terms of overall performance, as well as all splits apart from the many-shot case.<ul><li>To further justify our claim that it is beneficial to decouple representation and classifier, we experiment with fine-tuning the backbone network (ResNeXt-50) jointly with the linear classifier. In Table 1, we present results when fine-tuning the whole network with standard or smaller (0.1×) learning rate, fine-tuning only the last block in the backbone, or only retraining the linear classifier and fixing the representation. Fine-tuning the whole network yields the worst performance (46.3%and 48.8%), while keeping the representation frozen performs best (49.5%). The trend is even more evident for the medium/few-shot classes.</li></ul></li></ul><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-182421.png" alt=""></p><ul><li><strong>Instance-balanced sampling gives the most generalizable representations</strong>. This is particularly interesting, as it implies that <strong>data imbalance might not be an issue learning high-quality representations</strong>.</li></ul><h1>How to balance your classifier?</h1><p><strong>NCM v.s. cRT v.s. $\tau$-norm</strong></p><ul><li>The non-parametric <strong>NCM seems to perform slightly worse than cRT and $τ$-normalization</strong>. Those two methods are consistently better in most cases apart from the few-shot case, where NCM performs comparably.</li></ul><hr><p><strong>Weight norm and training data distribution visualization</strong></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-184406.png" alt=""></p><blockquote><p>Class index is sorted in a descending manner with respect to the number of instances in the training set.</p></blockquote><ul><li>We can observe that the weight norm of the joint classifier (blue line) is positively correlated with the number of training instances of the corresponding class. <strong>More-shot classes tend to learn a classifier with larger magnitudes</strong>. This yields a <strong>wider classification boundary</strong> in feature space, allowing the classifier to have much higher accuracy on data-rich classes, but hurting data-scarce classes.</li></ul><hr><p><strong>Accuracy with different values of the normalization parameter $τ$</strong></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-210307.png" alt=""></p><ul><li>The figure shows that as $τ$ increases from 0, many-shot accuracy decays dramatically while few-shot accuracy increases dramatically.</li></ul><h1>Comparison with the SOTA on long-tailed datasets</h1><p><strong>ImageNet-LT</strong></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-210307-1.png" alt=""></p><p><strong>iNaturalist 2018</strong></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-210307-2.png" alt=""></p><p><strong>Places-LT</strong></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-210307-3.png" alt=""></p><h1>On the exploration of determining $\tau$</h1><ul><li>The current tau-normalization strategy does require a validation set to choose tau, which could be a disadvantage depending on the practical scenario. Can we do better?</li><li><strong>Finding $τ$ value on training set</strong>. We also attempted to select $τ$ directly on the training dataset. Surprisingly, final performance on testing set is very similar, with $τ$ selected using training set only.<ul><li>We achieve this goal by simulating a balanced testing distribution from the training set. We first feed the whole training set through the network to get the top-1 accuracy for each of the classes. Then, we average the class-specific accuracies and use the averaged accuracy as the metric to determine the tau value. As shown in Table 9, we compare the $τ$ found on training set and validation set for all three datasets. We can see that both the value of $τ$ and the overall performances are very close to each other, which demonstrates the effectiveness of searching for $τ$ on training set.</li></ul></li><li><strong>Learning $τ$ value on training set</strong>. We further investigate if we can <strong>automatically learn the $τ$ value instead of grid search</strong>. To this end, following cRT, we set $τ$ as a learnable parameter and learn it on the training set with balanced sampling, while keeping all the other parameters fixed (including both the backbone network and classifier). Also, we compare the learned $τ$ value and the corresponding results in the Table 9. This further reduces the manual effort of searching best $τ$ values and make the strategy more accessible for practical usage.</li></ul><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-210307-4.png" alt=""></p><h1>References</h1><ul><li><a href="https://arxiv.org/abs/1910.09217">Decoupling Representation and Classifier for Long-Tailed Recognition, ICLR 2020</a></li><li>code: <a href="https://github.com/facebookresearch/classifier-balancing">https://github.com/facebookresearch/classifier-balancing</a></li><li><a href="https://zhuanlan.zhihu.com/p/158638078">长尾分布下分类问题的最新研究 (持续更新)</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>machine learning</category>
      
      <category>long-tailed problem</category>
      
    </categories>
    
    
    <tags>
      
      <tag>long-tailed problem</tag>
      
      <tag>re-sampling</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
