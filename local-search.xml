<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition</title>
    <link href="/2021/11/03/BBN-Bilateral-Branch-Network-with-Cumulative-Learning-for-Long-Tailed-Visual-Recognition/"/>
    <url>/2021/11/03/BBN-Bilateral-Branch-Network-with-Cumulative-Learning-for-Long-Tailed-Visual-Recognition/</url>
    
    <content type="html"><![CDATA[<p>这篇论文揭示了在长尾问题中，class re-balancing 策略虽然能有效促进 classifier 学习，但确会损害 feature extractor 的学习，为此提出了新的网络架构 BBN 及其对应的学习策略，用于在充分学习数据特征的同时也能关注到尾部数据。<span id="more"></span>值得一提的是，这篇论文得出的结论与 Decoupling Representation and Classifier for Long-Tailed Recognition 一致，而且还做了更为详尽的实验进行证明，这更加说明了这个结论的正确性。</p><h1>How class re-balancing strategies work?</h1><ul><li>Class re-balancing strategies promote classifier learning significantly but might <strong>damage the universal representative ability</strong> of the learned deep features due to distorting original distributions.</li></ul><hr><p><strong>Experiment</strong></p><ul><li>In order to justify our conjecture, we design a <strong>two-stage experimental fashion</strong> to separately learn representations and classifiers of deep models.<ul><li>Concretely, in the <strong>first stage</strong>, we train a classification network with plain training (i.e., cross-entropy) or re-balancing methods (i.e., re-weighting/re-sampling) as learning manners. Then, we obtain different kinds of feature extractors corresponding to these learning manners.</li><li>When it comes to the <strong>second stage</strong>, we fix the parameters of the feature extractors learned in the former stage, and retrain classifiers from scratch with the aforementioned learning manners again.</li></ul></li></ul><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/04/20211104-230252.png" alt=""></p><ul><li>By employing <strong>CE</strong> on the representation learning and employing <strong>RS</strong> on the classifier learning, we can achieve the lowest error rate on the validation set of CIFAR-100-IR50. (which <strong>coincides with <a href="https://ruikangliu.github.io/2021/11/02/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/">Decoupling Representation and Classifier for Long-Tailed Recognition</a>!</strong>)</li></ul><hr><p><strong>Affects of re-balancing strategies on the compactness of learned features</strong></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/04/20211104-230305.png" alt=""></p><ul><li>论文还进一步证明了 class re-balancing 的确会损害数据特征的学习。从下图可以看到，由 CE 和 RW 学得的类内特征分布更加分散，而直接由 CE 学得的类内特征分布则更加紧凑，这代表其学得的特征更有效</li></ul><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/04/20211104-230303.png" alt=""></p><h1>Bilateral-Branch Network (BBN)</h1><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/04/20211104-230309.png" alt=""></p><p><strong>Data samplers</strong></p><ul><li><p><strong>Uniform sampler</strong>: each sample in the training dataset is <strong>sampled only once</strong> with equal probability in a training epoch. The uniform sampler retains the characteristics of original distributions, and therefore benefits the representation learning.</p></li><li><p><strong>Reversed sampler</strong>: the sampling possibility of each class is proportional to the reciprocal of its sample size. There are three sub-procedures to construct the reversed sampler:</p><ol><li>Calculate the sampling possibility $P_i$ for class $i$ according to the number of samples as $$P_i=\frac{w_i}{\sum_{j=1}^Cw_j}$$where $w_i=\frac{N_{max}}{N_i}$</li><li>Randomly sample a class according to $P_i$</li><li>Uniformly pick up a sample from class $i$ with replacement.</li></ol></li></ul><p><strong>Weights sharing</strong></p><ul><li>Both branches use the same residual network structure and <strong>share all the weights except for the last residual block</strong>.  There are <strong>two benefits</strong> for sharing weights: On the one hand, the well-learned representation by the conventional learning branch can benefit the learning of the re-balancing branch. On the other hand, sharing weights will largely reduce computational complexity in the inference phase.</li></ul><p><strong>Cumulative learning strategy</strong></p><ul><li><strong>Cumulative learning strategy</strong> is proposed to shift the learning focus from the universal patterns to the tail data gradually by controlling both the weights for features produced by two branches and the classification loss $\mathcal L$.</li></ul><p>$$<br>\begin{aligned}<br>z&amp;=\alpha W_c^Tf_c+(1-\alpha)W_r^Tf_r\newline<br>\hat p_i&amp;=\frac{e^{z_i}}{\sum_{j=1}^Ce^{z_j}}\newline<br>\mathcal L &amp;= αE(\hat p, y_c) + (1 − α)E(\hat p, y_r)<br>\end{aligned}<br>$$<br>where $E(·, ·)$ denotes the cross-entropy loss function, and $\alpha$ will gradually decrease as the training epochs increasing:</p><p>$$<br>\alpha=1-\left(\frac{T}{T_{max}}\right)^2<br>$$</p><ul><li>Different from two-stage fine-tuning strategies, <strong>our $α$ ensures that both branches for different goals can be constantly updated in the whole training process, which could avoid the affects on one goal when it performs training for the other goal</strong>.</li></ul><p><strong>Inference phase</strong></p><ul><li>Because both branches are equally important, we simply fix $α$ to 0.5 in the test phase.</li></ul><blockquote><p><strong>BBN v.s. ensemble methods</strong></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/04/20211104-230312.png" alt=""></p></blockquote><h1>Experiments</h1><p><strong>Comparison methods</strong></p><ul><li><strong>Baseline methods</strong>. We employ plaining training with <strong>cross-entropy loss</strong> and focal loss as our baselines. Note that, we also conduct experiments with <strong>a series of mixup algorithms</strong> for comparisons.</li><li><strong>Two-stage fine-tuning strategies</strong>. We train networks with cross-entropy (CE) on imbalanced data in the first stage, and then conduct class re-balancing training in the second stage. “<strong>CE-DRW</strong>” and “<strong>CE-DRS</strong>” refer to the two-stage baselines using re-weighting and re-sampling at the second stage.</li><li><strong>State-of-the-art methods</strong>. For state-of-the-art methods, we compare with the recently proposed <strong>LDAM</strong> and <strong>CB-Focal</strong>.</li></ul><hr><p><strong>long-tailed CIFAR-10 and CIFAR-100</strong></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/04/20211104-230317.png" alt=""></p><p><strong>iNaturalist</strong></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/04/20211104-230320.png" alt=""></p><hr><p><strong>Ablation studies</strong></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/04/20211104-230324.png" alt=""></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/04/20211104-230326.png" alt=""></p><blockquote><p>由下图可以看到，相比其他的 decay 方法，Parabolic decay 在 $\alpha\geq0.5$ 的时间更多，也就是花了更多时间学习全局特征，这也是它表现最好的原因</p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/04/20211104-230328.png" alt=""></p></blockquote><hr><p><strong>Validation experiments of our proposals</strong></p><ul><li>Evaluations of feature quality. 给定一个训练好的 BBN 模型，固定 feature extractor 的参数，然后分别重新训练两个分支的 classifier，最后分别测试两个分支各自的错误率。可以看到，用于学习数据特征的 Conventional learning branch 取得的错误率接近直接用  vanilla CE 得到的错误率，说明它的确成功抽取出了全局的数据特征。同时，得益于参数共享，关注尾部数据的 Re-balancing 分支也能取得比重采样和重加权更好的效果</li></ul><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/04/20211104-230329.png" alt=""></p><ul><li>Visualization of classifier weights. 下图中，$\sigma$ 表示十个类别权重 $L_2$ 范数的标准差，“BBN-ALL” 表示 $W_c$ 和 $W_r$ 合并后的权重。可以看到，“BBN-ALL” 给各个类别的权重都基本相同，并且标准差是最低的；同时，“BBN-CB” 和 “BBN-RB” 的权重范数分布也的确与它们各自的职责相吻合</li></ul><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/04/20211104-230331.png" alt=""></p><h1>References</h1><ul><li><a href="https://arxiv.org/abs/1910.09217">BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition, CVPR 2020</a></li><li>code: <a href="https://github.com/Megvii-Nanjing/BBN">https://github.com/Megvii-Nanjing/BBN</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>machine learning</category>
      
      <category>long-tailed problem</category>
      
    </categories>
    
    
    <tags>
      
      <tag>long-tailed problem</tag>
      
      <tag>re-sampling</tag>
      
      <tag>CVPR 2020</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Decoupling Representation and Classifier for Long-Tailed Recognition</title>
    <link href="/2021/11/02/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/"/>
    <url>/2021/11/02/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/</url>
    
    <content type="html"><![CDATA[<ul><li>图片特征的分布和类别标注的分布本质上是不耦合的，任何不均衡分类数据集的再平衡本质上都应该只是对分类器的再均衡，而不应该用类别的分布改变特征学习时图片特征的分布。<span id="more"></span>也就是说，模型完全可以从类别不均衡的数据集中学得样本特征，类别的不均衡只会对分类器产生影响，re-sampling、re-weighting 等方法本质上都应该去平衡分类器的决策边界，而不应该去影响样本特征的学习</li></ul><h1>Introduction</h1><ul><li><p>One typical classifier includes a feature extractor (i.e. backbone) $f(x; θ) = z$ and a classifier $g(z) = W^Tz + b$, which can be formulated as $\tilde y = \arg \max g(z)$. Most of the existing solutions for long-tailed recognition adhere to the scheme of jointly learning representations ($\theta$) and classifiers ($W,b$). However, this paper devised a scheme that <strong>decouple the learning procedure into representation learning and classification</strong>, which set new <strong>SOTA</strong> on common long-tailed benchmarks like ImageNet-LT, Places-LT and iNaturalist. In the experiment, the authors also found surprising results that <strong>instance-balanced sampling learns the best and most generalizable representations</strong>, which challenges common beliefs.</p></li><li><p>What is more, this paper also proposed several <strong>learning methods for classification</strong> used in the decoupled learning scheme, including  1) re-training the parametric linear classifier in a class-balancing manner (i.e., <strong>re-sampling</strong>); 2) <strong>non-parametric nearest class mean classifier</strong>, which classifies the data based on their closest class-specific mean representations from the training set; and 3) <strong>normalizing the classifier weights</strong>, which adjusts the weight magnitude directly to be more balanced, adding a temperature to modulate the normalization procedure.</p></li></ul><h1>Learning Representations for long-tailed recognition</h1><ul><li><p><strong>Sampling strategies</strong>: Instance-balanced sampling, Class-balanced sampling, Square-root sampling, Progressively-balanced sampling</p></li><li><p><strong>Loss re-weighting strategies</strong></p></li></ul><h1>Classification for long-tailed recognition</h1><ul><li><p><strong>Classifier Re-training (cRT)</strong>: Re-train the classifier with class-balanced sampling. That is, keeping the representations fixed, we randomly re-initialize and optimize the classifier weights $W$ and $b$ for a small number of epochs using class-balanced sampling.</p></li><li><p><strong>Nearest Class Mean classifier (NCM)</strong>: First compute the mean feature representation for each class on the training set and then perform nearest neighbor search either using cosine similarity or the Euclidean distance computed on $L_2$ normalized mean features. Despite its simplicity, this is a strong baseline; the cosine similarity alleviates the weight imbalance problem via its inherent normalization.</p></li><li><p>$τ$-normalized classifier (<strong>$τ$-normalized</strong>). We investigate an efficient approach to re-balance the decision boundaries of classifiers, inspired by an empirical observation: After joint training with instance-balanced sampling, the norms of the weights $||w_j||$ are correlated with the cardinality of the classes $n_j$, while, after fine-tuning the classifiers using class-balanced sampling, the norms of the classifier weights tend to be more similar. <strong>Inspired by the above observations</strong>, we scale the weights of $W$ to get $\tilde W = {\tilde w_j }$ by:<br>$$<br>\tilde w_i=f_i*w_i,\text{where }f_i=\frac{1}{||w_i||^\tau}<br>$$<br>where $τ$ is a hyper-parameter controlling the “temperature” of the normalization.  We empirically choose $τ ∈ (0, 1)$ such that the weights can be rectified smoothly. After $τ$-normalization, the classification logits are given by $\hat y = \tilde W^Tf(x; θ)$. Note that we discard the bias term $b$ here due to its negligible effect on the logits and final predictions.</p></li><li><p><strong>Learnable weight scaling (LWS)</strong>: Although for $τ$-normalized in general $τ$ is chosen through cross-validation, we further investigate <strong>learning $f_i$ on the training set, using class-balanced sampling</strong>. In this case, we keep both the representations and classifier weights fixed and only learn the scaling factors $f_i$.</p></li></ul><blockquote><p><strong>Performance drops when a deeper classifier is used.</strong> (1 layer is the best) The author explains that it means the backbone network is enough to learn discriminative representation.</p></blockquote><h1>Experiments</h1><h2 id="Experiment">Experiment</h2><ul><li><strong>Datasets</strong>: Places-LT, ImageNet-LT, iNaturalist 2018.</li><li><strong>Evaluation Protocol</strong>: (1) $All$ (top-1 accuracy over all classes). To better examine performance variations across classes with different number of examples, we further report accuracy on three splits of the set of classes: (2) $Many$-$shot$ (more than 100 images), (3) $Medium$-$shot$ (20∼100 images) and (4) $Few$-$shot$ (less than 20 images).</li><li><strong>Implementation</strong>:<ul><li><strong>backbone</strong>:<ul><li>Places-LT: ResNet-152 (pretrained on ImageNet-2012)</li><li>ImageNet-LT: ResNet-{10,50,101,152} and ResNeXt-{50,101,152}(32x4d)</li><li>iNaturalist 2018: ResNet-{50,101,152}</li></ul></li><li><strong>others</strong>: SGD <strong>optimizer</strong> with momentum 0.9, batch size 512, cosine learning rate schedule (<a href="https://arxiv.org/abs/1608.03983v3">Loshchilov &amp; Hutter, 2016</a>) gradually decaying from 0.2 to 0 and <strong>image resolution</strong> 224 × 224. In the first representation learning stage, the backbone network is usually trained for 90 epochs. In the second stage, i.e., for retraining a classifier (cRT), we restart the learning rate and train it for 10 epochs while keeping the backbone network fixed.</li></ul></li></ul><h2 id="Sampling-strategies-and-decoupled-learning">Sampling strategies and decoupled learning</h2><ul><li>We first train models to learn representations with different sampling strategies. Next, we study three different basic approaches to obtain a classifier with balanced decision boundaries, on top of the learned representations.</li></ul><hr><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-180820.png" alt=""></p><p><strong>Key observations</strong>:</p><ul><li><strong>Sampling matters when training jointly</strong>. From the Joint results in Figure 1 across sampling methods and splits, we see consistent gains in performance when using better sampling strategies. The trends are consistent for the overall performance as well as the medium- and few-shot classes, with <strong>progressively-balanced sampling giving the best results</strong>.</li><li><strong>Decoupling representation and classifier is desirable for long-tailed recognition</strong>. For most cases presented in Figure 1, performance using decoupled methods is significantly better in terms of overall performance, as well as all splits apart from the many-shot case.<ul><li>To further justify our claim that it is beneficial to decouple representation and classifier, we experiment with fine-tuning the backbone network (ResNeXt-50) jointly with the linear classifier. In Table 1, we present results when fine-tuning the whole network with standard or smaller (0.1×) learning rate, fine-tuning only the last block in the backbone, or only retraining the linear classifier and fixing the representation. Fine-tuning the whole network yields the worst performance (46.3%and 48.8%), while keeping the representation frozen performs best (49.5%). The trend is even more evident for the medium/few-shot classes.</li></ul></li></ul><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-182421.png" alt=""></p><ul><li><strong>Instance-balanced sampling gives the most generalizable representations</strong>. This is particularly interesting, as it implies that <strong>data imbalance might not be an issue learning high-quality representations</strong>.</li></ul><h1>How to balance your classifier?</h1><p><strong>NCM v.s. cRT v.s. $\tau$-norm</strong></p><ul><li>The non-parametric <strong>NCM seems to perform slightly worse than cRT and $τ$-normalization</strong>. Those two methods are consistently better in most cases apart from the few-shot case, where NCM performs comparably.</li></ul><hr><p><strong>Weight norm and training data distribution visualization</strong></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-184406.png" alt=""></p><blockquote><p>Class index is sorted in a descending manner with respect to the number of instances in the training set.</p></blockquote><ul><li>We can observe that the weight norm of the joint classifier (blue line) is positively correlated with the number of training instances of the corresponding class. <strong>More-shot classes tend to learn a classifier with larger magnitudes</strong>. This yields a <strong>wider classification boundary</strong> in feature space, allowing the classifier to have much higher accuracy on data-rich classes, but hurting data-scarce classes.</li></ul><hr><p><strong>Accuracy with different values of the normalization parameter $τ$</strong></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-210307.png" alt=""></p><ul><li>The figure shows that as $τ$ increases from 0, many-shot accuracy decays dramatically while few-shot accuracy increases dramatically.</li></ul><h1>Comparison with the SOTA on long-tailed datasets</h1><p><strong>ImageNet-LT</strong></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-210307-1.png" alt=""></p><p><strong>iNaturalist 2018</strong></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-210307-2.png" alt=""></p><p><strong>Places-LT</strong></p><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-210307-3.png" alt=""></p><h1>On the exploration of determining $\tau$</h1><ul><li>The current tau-normalization strategy does require a validation set to choose tau, which could be a disadvantage depending on the practical scenario. Can we do better?</li><li><strong>Finding $τ$ value on training set</strong>. We also attempted to select $τ$ directly on the training dataset. Surprisingly, final performance on testing set is very similar, with $τ$ selected using training set only.<ul><li>We achieve this goal by simulating a balanced testing distribution from the training set. We first feed the whole training set through the network to get the top-1 accuracy for each of the classes. Then, we average the class-specific accuracies and use the averaged accuracy as the metric to determine the tau value. As shown in Table 9, we compare the $τ$ found on training set and validation set for all three datasets. We can see that both the value of $τ$ and the overall performances are very close to each other, which demonstrates the effectiveness of searching for $τ$ on training set.</li></ul></li><li><strong>Learning $τ$ value on training set</strong>. We further investigate if we can <strong>automatically learn the $τ$ value instead of grid search</strong>. To this end, following cRT, we set $τ$ as a learnable parameter and learn it on the training set with balanced sampling, while keeping all the other parameters fixed (including both the backbone network and classifier). Also, we compare the learned $τ$ value and the corresponding results in the Table 9. This further reduces the manual effort of searching best $τ$ values and make the strategy more accessible for practical usage.</li></ul><p><img src="https://lianlio-homepage.oss-cn-beijing.aliyuncs.com/img/post/2021/11/03/20211103-210307-4.png" alt=""></p><h1>References</h1><ul><li><a href="https://arxiv.org/abs/1910.09217">Decoupling Representation and Classifier for Long-Tailed Recognition, ICLR 2020</a></li><li>code: <a href="https://github.com/facebookresearch/classifier-balancing">https://github.com/facebookresearch/classifier-balancing</a></li><li><a href="https://zhuanlan.zhihu.com/p/158638078">长尾分布下分类问题的最新研究 (持续更新)</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>machine learning</category>
      
      <category>long-tailed problem</category>
      
    </categories>
    
    
    <tags>
      
      <tag>long-tailed problem</tag>
      
      <tag>re-sampling</tag>
      
      <tag>ICLR 2020</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
